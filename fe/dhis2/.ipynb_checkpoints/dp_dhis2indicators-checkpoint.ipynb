{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - DHIS2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Input data**: Data downloaded via API from DHIS2 System. This data is stored in *data/dhis2/data* directories\n",
    "- **Processing**:\n",
    "    - Getting Upazila and District level data\n",
    "    - Reading and processing and transforming data in the following form:\n",
    "        - Rows: *Geographical levels (upazila and districts)*\n",
    "        - Columns: *Variables/health indicators*\n",
    "        - Datasets: *Years [2009 - 2018]*\n",
    "- **Output data**: Processed data is stored in *data/output/dhis2/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def list_files(path):\n",
    "    \"\"\"\n",
    "    Function gets the list of all file from the target directory,\n",
    "    takes absolute path, splits file names to extract:\n",
    "    - File name\n",
    "    - Data element\n",
    "    - File type\n",
    "    - Year\n",
    "    - Geographical level/group\n",
    "    - Absolute path of the file name\n",
    "    Returns all this as a dataframe\n",
    "    \"\"\"\n",
    "    path_all = os.path.join(path, '*')\n",
    "    abs_file_names = glob.glob(path_all, recursive=True)\n",
    "    file_names = [os.path.split(glob.glob(file_path)[0])[1] for file_path in abs_file_names]\n",
    "    data_collection = [file_name.split(\"_\")[0] for file_name in file_names]\n",
    "    year = [file_name.split(\"_\")[1] for file_name in file_names]\n",
    "    geo_level = [file_name.split(\"_\")[2] for file_name in file_names]\n",
    "    file_type = [file_name[-8:-4] for file_name in file_names]\n",
    "    files_df = pd.DataFrame.from_dict({'file_names': file_names, \n",
    "                                       'data_collection': data_collection,\n",
    "                                       'year': year,\n",
    "                                       'geo_level': geo_level,\n",
    "                                       'file_type': file_type,\n",
    "                                       'abs_file_names': abs_file_names\n",
    "                                      })\n",
    "\n",
    "    return files_df\n",
    "\n",
    "def create_sets(input_df, geo_levels, years, file_types):\n",
    "    \"\"\"\n",
    "    input_df: Output from function list_files()\n",
    "    geo_levels: Defining geographic levels:\n",
    "                If as a list than it is a subset of geo levels from input_df\n",
    "                Otherwise it should ba string that specifies the column name\n",
    "    years: Defining years:\n",
    "           If as a list than it is a subset of geo levels from input_df\n",
    "           Otherwise it should ba string that specifies the column name\n",
    "    file_types: Either NAME or CODE. This is defined in the rawfiles.\n",
    "                NAME: Descriptive name of geographical unit as key ID variable\n",
    "                CODE: Geo-level code of geogrpahical unit as key ID variable\n",
    "    return: Dictionary by geo-level, year and file type\n",
    "    \"\"\"\n",
    "    df = input_df.copy(deep=True)\n",
    "    datasets = {}\n",
    "    if ((type(geo_levels) == list) & (type(years) == list)):\n",
    "        for geo_level in geo_levels:\n",
    "            for year in years:\n",
    "                for file_type in file_types:\n",
    "                    name = geo_level + '_' + year + '_' + file_type\n",
    "                    datasets[name] = df[(df['geo_level']==geo_level) &\n",
    "                                        (df['year']==year) & \n",
    "                                        (df['file_type']==file_type)] \n",
    "    else:\n",
    "        for geo_level in df[geo_levels].unique():\n",
    "            for year in df[years].unique():\n",
    "                for file_type in file_types:\n",
    "                    name = geo_level + '_' + year + '_' + file_type\n",
    "                    datasets[name] = df[(df['geo_level']==geo_level) & \n",
    "                                        (df['year']==year) & \n",
    "                                        (df['file_type']==file_type)]\n",
    "    return datasets\n",
    "\n",
    "def select_sets(datasets):\n",
    "    \"\"\"\n",
    "    datasets: Output from function create_sets\n",
    "    returns\n",
    "        - Data dictionary with loaded datasets by geo-level and year\n",
    "        # - Data frame with two columns:\n",
    "            # - List of file names with absolute paths\n",
    "            # - Logical column whether the loaded file is empty or not\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    # summary_df = {'filenames': list(), 'emptyness': list()}\n",
    "    for key, item in datasets.items():\n",
    "        data_dict[key] = dict()\n",
    "        for i in item.index:\n",
    "            tmp = pd.read_csv(item['abs_file_names'][i])\n",
    "            # summary_df['filenames'].append(os.path.split(item['abs_file_names'][i])[1])\n",
    "            # summary_df['emptyness'].append(tmp.empty)\n",
    "            data_dict[key][item['data_collection'][i]] = tmp\n",
    "    return data_dict # pd.DataFrame(summary_df)\n",
    "\n",
    "\n",
    "def extract_variables(data_dict):\n",
    "    \"\"\"\n",
    "    data_dict: Output from select_sets function\n",
    "    returns: Dataframe that summarizes all loaded files and includes following columns:\n",
    "        - variable_name: variable names if datasets is not empty otherwise it states 'No Variables'\n",
    "        - data_element: What data element from DHIS2 loaded datasets corresponds to\n",
    "        - is_empty: Is it empty or not\n",
    "        - geo_level: for example upazila or zilla\n",
    "        - year: 2009 - 2018\n",
    "    \"\"\"\n",
    "    output_dict = {'year':[], 'data_element':[], 'geo_level':[], 'var_name':[], 'is_empty':[]}\n",
    "    for key_dict, item_dict in data_dict.items():\n",
    "        for key, item in data_dict[key_dict].items():\n",
    "            if item.empty:\n",
    "                output_dict['var_name'].append('No Variables')\n",
    "                output_dict['data_element'].append(key)\n",
    "                output_dict['is_empty'].append(True)\n",
    "                output_dict['geo_level'].append(key_dict.split(\"_\")[1])\n",
    "                output_dict['year'].append(key_dict.split(\"_\")[0])\n",
    "            else:\n",
    "                for var in item['Data'].unique():\n",
    "                    output_dict['var_name'].append(var)\n",
    "                    output_dict['data_element'].append(key)\n",
    "                    output_dict['is_empty'].append(item.empty)\n",
    "                    output_dict['geo_level'].append(key_dict.split(\"_\")[1])\n",
    "                    output_dict['year'].append(key_dict.split(\"_\")[0])\n",
    "        return pd.DataFrame.from_dict(output_dict)\n",
    "\n",
    "    \n",
    "def transform_data(data_dict):\n",
    "    \"\"\"\n",
    "    data_dict: Output from function select_sets\n",
    "    returns: Dictionary (by geo-level and year) of pivoted dataframes which are not empty\n",
    "    \"\"\"\n",
    "    tdata_dict = {}\n",
    "    for key_dict, item_dict in data_dict.items():\n",
    "        tdata_dict[key_dict] = list()\n",
    "        for key, item in data_dict[key_dict].items():\n",
    "            if item.empty:\n",
    "                print('Provided dataframe is empty and therefore is not processed')\n",
    "            else:\n",
    "                tmp = pd.pivot_table(data=item, \n",
    "                                     values='Value', \n",
    "                                     index='Organisation unit',\n",
    "                                     columns='Data', \n",
    "                                     aggfunc='first')\n",
    "                tmp.columns = key + \": \" + tmp.columns\n",
    "                tdata_dict[key_dict].append(tmp)\n",
    "    return tdata_dict\n",
    "\n",
    "\n",
    "def merge_data(data_dict):\n",
    "    \"\"\"\n",
    "    data_dict: Output from transform_data function\n",
    "    returns: Dictionary of merged datasets by year and geo-level\n",
    "    \"\"\"\n",
    "    output_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        tmp_list = sorted(value, key=len, reverse=True)\n",
    "        output_dict[key] = tmp_list[0].join(tmp_list[1:])\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def write_data(data_dict, path):\n",
    "    \"\"\"\n",
    "    data_dict: Output from merge_data function\n",
    "    Writes out all dictionary elements as csv files\n",
    "    \"\"\"\n",
    "    for key, value in data_dict.items():\n",
    "        value.to_csv(os.path.join(path,key + '.csv'))\n",
    "        print(f\"Writing {key} to {os.path.join(path,key + '.csv')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/edinhamzic/Symphony/wb_bangladesh/data/dhis2/data'\n",
    "WD = '/Users/edinhamzic/Symphony/wb_bangladesh/'\n",
    "OUT = '/Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>data_collection</th>\n",
       "      <th>year</th>\n",
       "      <th>geo_level</th>\n",
       "      <th>file_type</th>\n",
       "      <th>abs_file_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04EPIDistrictRequisition_2011_UpazilaandDistri...</td>\n",
       "      <td>04EPIDistrictRequisition</td>\n",
       "      <td>2011</td>\n",
       "      <td>UpazilaandDistrictlevelHF</td>\n",
       "      <td>NAME</td>\n",
       "      <td>/Users/edinhamzic/Symphony/wb_bangladesh/data/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02EPIUpazilaStock_2010_UpazilaHealthComplex_CO...</td>\n",
       "      <td>02EPIUpazilaStock</td>\n",
       "      <td>2010</td>\n",
       "      <td>UpazilaHealthComplex</td>\n",
       "      <td>CODE</td>\n",
       "      <td>/Users/edinhamzic/Symphony/wb_bangladesh/data/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10KMC_2011_DistrictNGO&amp;PrivateTotal_CODE.csv</td>\n",
       "      <td>10KMC</td>\n",
       "      <td>2011</td>\n",
       "      <td>DistrictNGO&amp;PrivateTotal</td>\n",
       "      <td>CODE</td>\n",
       "      <td>/Users/edinhamzic/Symphony/wb_bangladesh/data/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11EMEN(MBFFI)_2012_DistrictNGO&amp;PrivateTotal_CO...</td>\n",
       "      <td>11EMEN(MBFFI)</td>\n",
       "      <td>2012</td>\n",
       "      <td>DistrictNGO&amp;PrivateTotal</td>\n",
       "      <td>CODE</td>\n",
       "      <td>/Users/edinhamzic/Symphony/wb_bangladesh/data/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07EmONC_2009_UpazilaandDistrictlevelHF_NAME.csv</td>\n",
       "      <td>07EmONC</td>\n",
       "      <td>2009</td>\n",
       "      <td>UpazilaandDistrictlevelHF</td>\n",
       "      <td>NAME</td>\n",
       "      <td>/Users/edinhamzic/Symphony/wb_bangladesh/data/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file_names  \\\n",
       "0  04EPIDistrictRequisition_2011_UpazilaandDistri...   \n",
       "1  02EPIUpazilaStock_2010_UpazilaHealthComplex_CO...   \n",
       "2       10KMC_2011_DistrictNGO&PrivateTotal_CODE.csv   \n",
       "3  11EMEN(MBFFI)_2012_DistrictNGO&PrivateTotal_CO...   \n",
       "4    07EmONC_2009_UpazilaandDistrictlevelHF_NAME.csv   \n",
       "\n",
       "            data_collection  year                  geo_level file_type  \\\n",
       "0  04EPIDistrictRequisition  2011  UpazilaandDistrictlevelHF      NAME   \n",
       "1         02EPIUpazilaStock  2010       UpazilaHealthComplex      CODE   \n",
       "2                     10KMC  2011   DistrictNGO&PrivateTotal      CODE   \n",
       "3             11EMEN(MBFFI)  2012   DistrictNGO&PrivateTotal      CODE   \n",
       "4                   07EmONC  2009  UpazilaandDistrictlevelHF      NAME   \n",
       "\n",
       "                                      abs_file_names  \n",
       "0  /Users/edinhamzic/Symphony/wb_bangladesh/data/...  \n",
       "1  /Users/edinhamzic/Symphony/wb_bangladesh/data/...  \n",
       "2  /Users/edinhamzic/Symphony/wb_bangladesh/data/...  \n",
       "3  /Users/edinhamzic/Symphony/wb_bangladesh/data/...  \n",
       "4  /Users/edinhamzic/Symphony/wb_bangladesh/data/...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dhis2_files = list_files(DATA_PATH)\n",
    "display(dhis2_files.head())\n",
    "dhis2_geos = ['Upazila', 'District']\n",
    "dhis2_years = ['2009','2010','2011','2012','2013','2014','2015','2016','2017', '2018']\n",
    "dhis2_names = ['NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating, selecting/reading, summarizing, transforming and merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n",
      "Provided dataframe is empty and therefore is not processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating sets\n",
    "datasets_dict = create_sets(dhis2_files, geo_levels=dhis2_geos, years=dhis2_years, file_types=dhis2_names)\n",
    "\n",
    "# Selecting and reading files\n",
    "datasets= select_sets(datasets_dict)\n",
    "\n",
    "# Summarizing datasets\n",
    "data_summary = extract_variables(datasets)\n",
    "\n",
    "# Transform and merge\n",
    "data = merge_data(transform_data(datasets))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upazilla_epi_Syringe (0.05 ml) previous stock\n",
      "upazilla_epi_MR vial previous stock\n",
      "upazilla_epi_Safety Box_used\n",
      "upazilla_epi_Child register_damaged\n",
      "upazilla_epi_MR vial_used\n",
      "upazilla_epi_Safety Box previous stock\n",
      "upazilla_epi_Women Report Book_damaged\n",
      "upazilla_epi_Moni_Flag_previous_stock\n",
      "upazilla_epi_TT Card_used\n",
      "upazilla_epi_Child Card_used\n",
      "upazilla_epi_PCV vial_damaged\n",
      "upazilla_epi_BCG vial_damaged\n",
      "upazilla_epi_Women register_receive\n",
      "upazilla_epi_Syringe (3 ml)_damaged\n",
      "upazilla_epi_Syringe (3 ml) previous stock\n",
      "upazilla_epi_Tally Book (Women)_used\n",
      "upazilla_epi_MR Diluent_damaged\n",
      "upazilla_epi_TT Card_received\n",
      "upazilla_epi_OPV vial_receive\n",
      "upazilla_epi_Syringe (3 ml)_used\n",
      "upazilla_epi_Child Card_receive\n",
      "upazilla_epi_PCV vial_receive\n",
      "upazilla_epi_Moni Flag_receive\n",
      "upazilla_epi_IPV vial previous stock\n",
      "upazilla_epi_TT Card_damaged\n",
      "upazilla_epi_Tally Book (child)_damaged\n",
      "upazilla_epi_Child register previous stock\n",
      "upazilla_epi_TT vial_receive\n",
      "upazilla_epi_IPV vial_supplied1\n",
      "upazilla_epi_Syringe (5 ml)_damaged\n",
      "upazilla_epi_BCG vial_receive\n",
      "upazilla_epi_Syringe (0.5 ml)_used\n",
      "upazilla_epi_Syringe (0.1 ml)_receive\n",
      "upazilla_epi_Pentavalent vial_damaged\n",
      "upazilla_epi_IPV vial_receive\n",
      "upazilla_epi_Syringe (3 ml)_receive\n",
      "upazilla_epi_Syringe (0.5 ml)_receive\n",
      "upazilla_epi_BCG vial_used\n",
      "upazilla_epi_PCV vial previous stock\n",
      "upazilla_epi_Syringe (0.05 ml)_receive\n",
      "upazilla_epi_Women register_used\n",
      "upazilla_epi_BCG vial previous stock\n",
      "upazilla_epi_BCG Diluent_used\n",
      "upazilla_epi_Pentavalent vial_used\n",
      "upazilla_epi_PCV need\n",
      "upazilla_epi_Women register_damaged\n",
      "upazilla_epi_Syringe (0.05 ml)_damaged\n",
      "upazilla_epi_TT vial_used\n",
      "upazilla_epi_Pentavalent vial previous stock\n",
      "upazilla_epi_Child register_used\n",
      "upazilla_epi_Child Report Book_used\n",
      "upazilla_epi_Women Report Book_receive\n",
      "upazilla_epi_TT vial_damaged\n",
      "upazilla_epi_Syringe (0.1 ml)_used\n",
      "upazilla_epi_MR Diluent_used\n",
      "upazilla_epi_Safety Box_damaged\n",
      "upazilla_epi_BCG Diluent previous stock\n",
      "upazilla_epi_TT vial previous stock\n",
      "upazilla_epi_Tally Book (child) previous stock\n",
      "upazilla_epi_MR Diluent previous stock\n",
      "upazilla_epi_Tally Book (Women)_receive\n",
      "upazilla_epi_Syringe (5 ml)_receive\n",
      "upazilla_epi_Tally Book (child)_used\n",
      "upazilla_epi_TT Card previous stock\n",
      "upazilla_epi_Women Report Book_used\n",
      "upazilla_epi_Syringe (0.5 ml) previous stock\n",
      "upazilla_epi_OPV vial previous stock\n",
      "upazilla_epi_Tally Book (child)_receive\n",
      "upazilla_epi_Syringe (0.5 ml)_damaged\n",
      "upazilla_epi_BCG vial_lot number1\n",
      "upazilla_epi_Women register previous stock\n",
      "upazilla_epi_BCG Diluent_damaged\n",
      "upazilla_epi_Women Report Book previous stock\n",
      "upazilla_epi_Tally Book (Women) previous stock\n",
      "upazilla_epi_Pentavalent vial_receive\n",
      "upazilla_epi_Child Card previous stock\n",
      "upazilla_epi_PCV vial_used\n",
      "upazilla_epi_Safety Box_receive\n",
      "upazilla_epi_PCV vial_supplied1\n",
      "upazilla_epi_MR Diluent_receive\n",
      "upazilla_epi_Child Report Book_receive\n",
      "upazilla_epi_Child Report Bookprevious stock\n",
      "upazilla_epi_Tally Book (Women)_damaged\n",
      "upazilla_epi_Syringe (5 ml)_used\n",
      "upazilla_epi_Child register_receive\n",
      "upazilla_epi_BCG Diluent_receive\n",
      "upazilla_epi_Moni Flag_damaged\n",
      "upazilla_epi_OPV vial_damaged\n",
      "upazilla_epi_Syringe (0.05 ml)_used\n",
      "upazilla_epi_Child Card_damaged\n",
      "upazilla_epi_Moni Flag_used\n",
      "upazilla_epi_IPV vial_used\n",
      "upazilla_epi_Syringe (0.1 ml) previous stock\n",
      "upazilla_epi_Syringe (5 ml) previous stock\n",
      "upazilla_epi_MR vial_damaged\n",
      "upazilla_epi_Child Report Book_damaged\n",
      "upazilla_epi_MR vial_receive\n",
      "upazilla_epi_OPV vial_used\n",
      "upazilla_epi_Measles Diluent_damaged\n",
      "upazilla_epi_Measles vial previous stock\n",
      "upazilla_epi_Measles Diluent previous stock\n",
      "upazilla_epi_Measles vial_used\n",
      "upazilla_epi_Measles vial_receive\n",
      "upazilla_epi_Measles vial_damaged\n",
      "upazilla_epi_Vitamin A Capsule previous stock\n",
      "upazilla_epi_Vitamin A Capsule_damaged\n",
      "upazilla_epi_Measles Diluent_used\n",
      "upazilla_epi_Vitamin A Capsule_receive\n",
      "upazilla_epi_Vitamin A Capsule_used\n"
     ]
    }
   ],
   "source": [
    "for var in data_summary[data_summary['data_element'] == '02EPIUpazilaStock']['var_name']:\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['03EPIDistrictStock', '05EPIDistrictSupply', '10KMC',\n",
       "       '09AdolescentHealth', '01EPIReport(Routinevaccination)',\n",
       "       '04EPIDistrictRequisition', '08IMCI', '11EMEN(MBFFI)',\n",
       "       '02EPIUpazilaStock', '06EPIUpazilaSupply', '07EmONC'], dtype=object)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_summary.data_element.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Upazila_2009_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2009_NAME.csv\n",
      "Writing Upazila_2010_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2010_NAME.csv\n",
      "Writing Upazila_2011_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2011_NAME.csv\n",
      "Writing Upazila_2012_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2012_NAME.csv\n",
      "Writing Upazila_2013_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2013_NAME.csv\n",
      "Writing Upazila_2014_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2014_NAME.csv\n",
      "Writing Upazila_2015_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2015_NAME.csv\n",
      "Writing Upazila_2016_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2016_NAME.csv\n",
      "Writing Upazila_2017_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2017_NAME.csv\n",
      "Writing Upazila_2018_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/Upazila_2018_NAME.csv\n",
      "Writing District_2009_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2009_NAME.csv\n",
      "Writing District_2010_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2010_NAME.csv\n",
      "Writing District_2011_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2011_NAME.csv\n",
      "Writing District_2012_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2012_NAME.csv\n",
      "Writing District_2013_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2013_NAME.csv\n",
      "Writing District_2014_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2014_NAME.csv\n",
      "Writing District_2015_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2015_NAME.csv\n",
      "Writing District_2016_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2016_NAME.csv\n",
      "Writing District_2017_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2017_NAME.csv\n",
      "Writing District_2018_NAME to /Users/edinhamzic/Symphony/wb_bangladesh/output/dhis2/data/District_2018_NAME.csv\n"
     ]
    }
   ],
   "source": [
    "write_data(data_dict=data, path=OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_summary.to_csv(os.path.join(OUT, 'data_summary.csv'), index=False, index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
